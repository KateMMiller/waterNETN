---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 10)
```

```{css echo = FALSE}
.indent {
margin-left: 25px;
font-size: 14px;
}

.indent2 {
margin-left: 50px;
font-size: 12px;
}

.drop{
  font-family: "Arial",Arial,sans-serif;
  font-size: 16px;
  font-weight: bold;
  padding:0px 0px 0px 0px;
  margin:0px 0px 0px 0px;
}
```

```{r echo = F, include = F}
print_head <- function(df){
  knitr::kable(df[1:6,]) |> #, table.attr = "style='width:60%;'") |> 
    kableExtra::kable_classic(full_width = F, font_size = 12, 
                              bootstrap_options = c("condensed"))
}
```

## Using the waterNETN R package {.tabset .tabset-pills}

### Getting started

<h4><b>Step 1.</b> Install R, RStudio, and RTools in Software Center</h4>
<h4><b>Step 2.</b> Install devtools package in R:</h4>
```{r, eval = F, class.source = "indent"}
install.packages('devtools')
```
<h4><b>Step 3.</b> Install waterNETN from github</h4>
Note that whenever the `waterNETN` package is updated, you can rerun this code to install the latest version.
```{r, eval = F, class.source = "indent"}
library(devtools)
install_github("KateMMiller/waterNETN")
```
<h4><b>Step 4.</b> Load waterNETN R package</h4>
```{r, class.source = "indent"}
library(waterNETN)
```
<h4><b>Step 5.</b> Import data</h4>
Note that R is not able to connect to files on Sharepoint or MS Teams (b/c Teams also stores all files on Sharepoint). That means you need to store data package files on your local machine or on a server (e.g. NETN Z drive).
```{r echo = F, results = 'hide', include = F}
importData()
```
<p class = 'indent'>
<b>Option 1.</b> Import data via .csv files. The file path should be where csvs are on your machine or server.</p>
```{r, eval = F, class.source = "indent"}
importData(type = 'csv',
           filepath = "C:/NETN/R_Dev/Water/data") # update filepath to your computer
```
<p class = 'indent'>
<b>Option 2.</b> Import data via zip file of csvs. The filepath should be the location and name of the zip file.</p>
```{r eval = F, class.source = "indent"}
importData(type = 'zip',
           filepath = "C:/NETN/R_Dev/Water/data/NETN_Water_Data_Package_20240423.zip")
```
<p class = 'indent'>
<b>Option 3.</b> Import data via data package database file on your computer</p>
```{r eval = F, class.source = "indent"}
importData(type = 'dbfile',
           filepath = "C:/NETN/R_Dev/Water/data/NETN_h3Ov4_DataPackage_202331115.accdb")
```
<p class = 'indent'>
<b>Option 4.</b> Import data via data package database DSN (Data Source Name) on your computer. Note that this is the default setting. As long as you have a named DSN called "NETNWQ_DP" that links to the data package database, and that database links to the latest NETN WQ backend database, the code below will run. See <i>Setting up DSN</i> tab for how to set up DSN.</p>
```{r eval = F, class.source = "indent"}
importData() # easiest
importData(type = 'DSN', odbc = "NETNWQ_DP") # equivalent to line above
```
<h4><b>Step 6.</b> (Optional) Export data package to zip </h4>
You can export all of the csvs to a zip file with the day's date stamped on the file name. This allows you to import the tables from the database, then export the csvs as one zip file.
```{r eval = F, echo = T, class.source = "indent"}
importData() # easiest
exportData(filepath = "./data", zip = TRUE) 
```

```{r eval = F, echo = F, class.source = "indent"}
importData() # easiest
exportData(filepath = "../data", zip = TRUE) 
```


### Getting help
<h4><b>Getting (and improving) help</b></h4>
The functions in `waterNETN` have help documentation like any R package. To view the help, you can go to the Packages tab and click on waterNETN (see below). That will show you all the functions in the package. Clicking on individual functions will take you to the help documentation for that function. 
```{r echo = F, out.height = 150, fig.align = 'center'}
knitr::include_graphics("C:/NETN/R_Dev/Water/waterNETN/testing_scripts/finding_R_package.jpg")
```

You can also see the help of a function by running, for example: 
```{r, class.source = 'indent', eval = F, echo = T}
?importData
```

If `waterNETN` isn't loaded yet, you'd run: 
```{r, class.source = 'indent', eval = F, echo = T}
?waterNETN::importData
```

Each function's help includes a Description, Usage (i.e. function arguments and their defaults), Argument options/definitions, and several examples showing how the function can be used. 

<b><span style='color:red;'>This is where you come in! If you notice typos or can think of better descriptions, examples, error messages, etc., please send them my way!</b></span> After we're more comfortable with R packages and get versed on GitHub, you'll be able to make those changes directly in the package. For now, you can just send me your suggestions and I'll make the changes.

Finally, if you ever want to peak under the hood at the function, you can view it several ways. 
<ol>
<li>Keep F2 key pressed and click on the function name in R. This trick works for many but not all functions in R.</li>
<li>View code in the <a href="https://github.com/KateMMiller/waterNETN/tree/main">GitHub katemmiller/waterNETN repo</a>. The functions are in the R folder. 

### Setting up DSN
<h4><b>Setting up a DSN</b></h4>
<ol>
<li>Go to Windows Start Menu and search ODBC. Click on ODBC Data Sources (64-bit) </li>

```{r echo = F, out.height = 350, fig.align = 'center'}
knitr::include_graphics("C:/NETN/R_Dev/Water/waterNETN/testing_scripts/ODBC_step_1.jpg")
```
<li>Click on <i>Add</i>, then select <i>Microsoft Access Driver (\*.mdb, \*.accdb)</i> then click <i>Finish</i> in next menu.</li>

```{r echo = F, out.height = 400, fig.align = 'center'}
knitr::include_graphics("C:/NETN/R_Dev/Water/waterNETN/testing_scripts/ODBC_step_2.jpg")
```

<li>Enter <b>NETNWQ_DP</b> into the Data Source Name (red arrow), click on Select (red circle). In new window, click on C:/ (orange arrow) and find the path to your database. If it's on the Z drive, then click on the Drives window and select correct Drive. Click on the data package database (yellow arrow). When complete, click OK. Finally, add the name of the database file to the Description (blue arrow), so it's easier to check whether you're using the latest version.  </li>
```{r echo = F, out.height = 400, fig.align = 'center'}
knitr::include_graphics("C:/NETN/R_Dev/Water/waterNETN/testing_scripts/ODBC_step_3.jpg")
```

<li>If updating an existing DSN, follow similar process, except click on the DSN in the first window and select <i>Configure</i> instead of Add.</li>
</ol>

### Climate Data
<details open><summary class = 'drop'>Daymet</summary>
Daymet data are 1km gridded daily climate data, similar to PRISM data, but with more daily data associated with each location and more efficient downloading process (extract data from lat/long points, rather than download nationwide raster files and extract manually). Daymet data go back to 1980. See <a href="https://daymet.ornl.gov/">https://daymet.ornl.gov/</a> for more information. Another benefit of Daymet over PRISM is the data can be used to calculate drought indices, like Standardized Precipitation-Evapotranspiration Index (SPEI). Note: I'll eventually add SPEI metrics to the output of this function. 

Daymet data are typically available for a calendar year a a few months into the next year.

In waterNETN, you can download Daymet data by park, site, site type, and spanning years 1980 to 2023. See examples below. The larger the number of sites and years, the longer it can take. I added the option to save the data as a csv to disk via `export = T` and specifying a `filepath`. I'll make it possible to read csvs into R to work with climate summary and plotting functions.  

The following code downloads Daymet data for East Primrose Brook in MORR for the entire monitoring period, and exports to a data folder in the working directory.
```{r, echo = T, eval = F}
prim <- getClimDaymet(site = "MORRSA", years = 2006:2023, export = T, filepath = "./data")
print_head(prim) # table of first 6 rows- this is a custom function I wrote to save on coding
```
```{r, echo = F, eval = T}
prim <- getClimDaymet(site = "MORRSA", years = 2006:2023, export = T, filepath = "../data")
print_head(prim) # printing condensed head of the data
```

The following code downloads Daymet data for all sites in MABI for 2023. Results are assigned to R session environment and not exported to data folder in the working directory.
```{r, echo = T}
# Daymet data for the Pogue and Pogue stream for 2023, but not exporting to disk.
mabi_dm <- getClimDaymet(park = "MABI", years = 2023)
print_head(mabi_dm)
```
</details>
<br>
<details open><summary class = 'drop'>Weather Station</summary>
For each NETN monitoring site, I used the `rnoaa` package to find all weather stations within 20km radius of each site's lat/long coordinates. I then checked the period of record for each of these sites. I selected the closest weather station with the longest period of record that included 2023 and 2024. This assessment was done at the park level, so the data for each site within a park is the same. The API used to download weather station data is <a href="data.rcc-acis.org">data.rcc-acis.org</a>. Unfortunately only temperature data are available for ACAD's McFarland Hill weather station (AKA MARS). ACAD's hourly preciptation data were downloaded separately from the NADP website <a href="http://nadp2.slh.wisc.edu/siteOps/ppt/">http://nadp2.slh.wisc.edu/siteOps/ppt/</a>. These data only go back to 2008 and were aggregated to daily total precipitation. This function is slower for ACAD as a result. Though note that data are downloaded at the park level, then joined with site-specific data. In other words, data are downloaded for a park x combination of years once, then joined to site-level data. Weather stations used and their distance from each site are included in the output.

The following code downloads weather station data for all sites in ROVA during the monitoring period and writes to a csv.
```{r echo = T, eval = F}
rova_ws <- getClimWStat(park = "ROVA", years = 2006:2023, export = T, filepath = "./data")
print_head(rova_ws)
```

```{r echo = F, eval = T}
rova_ws <- getClimWStat(park = "ROVA", years = 2006:2023, export = T, filepath = "../data")
print_head(rova_ws)
```

The following code downloads weather station data for all lakes in ACAD in 2023
```{r echo = T}
acad_ws <- getClimWStat(park = "ACAD", site_type = "lake", years = 2023)
print_head(acad_ws)
```
</details>
<br>

### Data Package Functions {.tabset}
#### getSites
#### getEvents
#### getChemistry

